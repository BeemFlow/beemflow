{
  "name": "openai",
  "description": "Call OpenAI's Chat Completions API (v1/chat/completions) for LLM and function-calling tasks.",
  "kind": "task",
  "parameters": {
    "type": "object",
    "required": ["model", "api_key", "messages"],
    "properties": {
      "model": { "type": "string", "description": "OpenAI model name (e.g. gpt-4o, gpt-4, gpt-3.5-turbo)" },
      "api_key": { "type": "string", "description": "OpenAI API key (use {{secrets.OPENAI_API_KEY}})", "default": { "$env": "OPENAI_API_KEY" } },
      "messages": {
        "type": "array",
        "description": "Chat messages (role: system|user|assistant, content: string)",
        "items": {
          "type": "object",
          "required": ["role", "content"],
          "properties": {
            "role": { "type": "string", "enum": ["system", "user", "assistant"] },
            "content": { "type": "string" }
          }
        }
      },
      "functions": {
        "type": "array",
        "description": "List of functions the model may call, defined using JSON Schema.",
        "items": {
          "type": "object",
          "required": ["name", "description", "parameters"],
          "properties": {
            "name": { "type": "string" },
            "description": { "type": "string" },
            "parameters": { "type": "object", "description": "JSON Schema object for function arguments" }
          }
        }
      },
      "function_call": {
        "oneOf": [
          { "type": "string", "enum": ["none", "auto"] },
          {
            "type": "object",
            "properties": {
              "name": { "type": "string" }
            },
            "required": ["name"]
          }
        ],
        "description": "Controls automatic function calling: none, auto, or specify function name"
      },
      "temperature": {
        "type": "number",
        "minimum": 0,
        "maximum": 2,
        "default": 1.0,
        "description": "What sampling temperature to use, between 0 and 2"
      },
      "top_p": {
        "type": "number",
        "minimum": 0,
        "maximum": 1,
        "default": 1.0,
        "description": "An alternative to sampling with temperature, called nucleus sampling"
      },
      "n": {
        "type": "integer",
        "minimum": 1,
        "default": 1,
        "description": "How many chat completion choices to generate"
      },
      "stream": {
        "type": "boolean",
        "default": false,
        "description": "Whether to stream back partial progress"
      },
      "stop": {
        "oneOf": [
          { "type": "string" },
          { "type": "array", "items": { "type": "string" } }
        ],
        "description": "Sequences where the API will stop generating further tokens"
      },
      "max_tokens": {
        "type": "integer",
        "description": "The maximum number of tokens to generate in the completion"
      },
      "presence_penalty": {
        "type": "number",
        "description": "Number between -2.0 and 2.0, penalizing new tokens based on whether they appear in the text so far"
      },
      "frequency_penalty": {
        "type": "number",
        "description": "Number between -2.0 and 2.0, penalizing new tokens based on their existing frequency in the text so far"
      },
      "logit_bias": {
        "type": "object",
        "additionalProperties": { "type": "number" },
        "description": "Modify the likelihood of specified tokens appearing in the completion"
      },
      "user": {
        "type": "string",
        "description": "A unique identifier representing your end-user"
      }
    }
  }
} 